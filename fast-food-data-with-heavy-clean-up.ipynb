{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bencaiello/fast-food-data-with-heavy-clean-up?scriptVersionId=141005005\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.391284Z","iopub.execute_input":"2023-08-25T15:27:04.391715Z","iopub.status.idle":"2023-08-25T15:27:04.40097Z","shell.execute_reply.started":"2023-08-25T15:27:04.391679Z","shell.execute_reply":"2023-08-25T15:27:04.399838Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Look\nNow that the standard Kaggle import is done, let's open the file and look at it a bit\n\nNote that some lines of code are commented out in this notebook (usually .info(), head(), or other similar calls that supply some information about the dataframe before/after changes) which I did to declutter the output after each cell.","metadata":{}},{"cell_type":"code","source":"#load file\nfile = pd.read_csv('/kaggle/input/fast-food/FastFoodNutritionMenu.csv')\n\n#exploratory examination of file\ndisplay(file.head())\nfile.info()\n#file.isna().sum()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.404057Z","iopub.execute_input":"2023-08-25T15:27:04.405273Z","iopub.status.idle":"2023-08-25T15:27:04.446021Z","shell.execute_reply.started":"2023-08-25T15:27:04.40522Z","shell.execute_reply":"2023-08-25T15:27:04.444752Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting columns to numeric: checking what doesn't fit\nAll columns except Company / Item should be numeric (I will use float type) but are currently object type, so we will need to eventually convert them to float.\n\nAt the same time, I also want to know what is in my columns that aren't numeric, so I wrote a function that will reveal what kinds of values cannot be converted to float in each of these columns, so that I can see what other cleaning steps might be used to salvage data / determine whether other errors may be present:","metadata":{}},{"cell_type":"code","source":"def df_non_numeric_list(dataframe,column_exclusion_list=None):\n    \"\"\" Print all unique values in a dataframe, by column, that cannot be converted to a float.\n    \n    Parameters\n    ----------\n    dataframe (pandas dataframe): required, the dataframe whose columns are to be analyzed for non-numeric convertible values\n    column_exclusion_list (list): optional, a list of column names of those columns to be excluded from the analysis. Default value is None.\n    \n    Returns\n    -------\n    Prints: the name of each column in the dataframe, followed by a list of all the non-numeric convertible values in the column.\n    \"\"\"\n    for column in dataframe.columns:\n        error_list = []\n        if column in column_exclusion_list:\n            pass\n        else:\n            for element in dataframe[column]:\n                try:\n                    element_float = float(element)\n                except:\n                    error_list.append(element)\n            print(column , ': ', pd.Series(error_list).unique())\n\n#Check what non-numerics are hiding in the columns that ought to be numeric....\ndf_non_numeric_list(file,['Company','Item'])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.448465Z","iopub.execute_input":"2023-08-25T15:27:04.448853Z","iopub.status.idle":"2023-08-25T15:27:04.46691Z","shell.execute_reply.started":"2023-08-25T15:27:04.448821Z","shell.execute_reply":"2023-08-25T15:27:04.465573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The non-numeric values in the numerical columns: how to fix the easy ones\nI found that the values in a non numeric-convertable form were either:\n  \n  '\\xa0', \n \n a column name (which usually did not match the parent column),\n \n '<1' or '<5', \n \n plus one instance where the units were included with a usable number ('5.5 g')","metadata":{}},{"cell_type":"code","source":"#Simple cleaning\n#Step 1: clean the '5.5g' data point in the saturated fat column\nfile['Saturated Fat\\n(g)'] = file['Saturated Fat\\n(g)'].str.replace('5.5 g','5.5')\n\n#Step 2: convert all '\\xa0' to NaN and all '<1' / '<5' to 0:\nfile_no_company_item = file.drop(['Company','Item'],axis=1)\nfor i in file_no_company_item.columns:\n    change_list = []\n    for j in file[i]:\n        if j == '\\xa0':\n            change_list.append(np.nan)\n        elif (j == '<1') | (j == '<5'):\n            change_list.append(0)\n        else:\n            change_list.append(j) \n    file[i] = pd.Series(change_list)\n\n#Confirm that the '<1','<5', and '5.5 g' are no longer in the dataframe using my previously defined function:\nprint('After fixing a number of the non-convertible items in the numeric columns: re-test to confirm they have been fixed: \\n')\ndf_non_numeric_list(file,['Company','Item'])\n#so now everything can be converted to float, except those strange column labels, which are also oddly in the wrong columns!\n\n#check for duplicates, and remove (there are 7 duplicates, just like I saw another notebook find)\nprint(len(file))\nfile = file.drop_duplicates()\nprint(len(file))\n\n#remove all rows where every value is NaN (except company/item). Examination of the csv in \n#excel (downloaded, personal computer) showed a good number of rows with only missing / 0 values.\n#There is no point is keeping these rows or imputing any values to them as they contain no data.\n#However, before I delete them I also want to save what items are being removed\nfile_no_company_item = file.drop(['Company','Item'],axis = 1)\n\nfile_company_item_only = file[['Company','Item']]\nfile_company_item_only_index = set(file.index)\n\nfile = file.dropna(subset = file_no_company_item.columns, how='all')\n\n#uncomment to see the company/items removed by dropping \"all NaN\" rows\n#removed_items = list(file_company_item_only_index.difference(set(file.index)))\n#for i in removed_items:\n#    print(file_company_item_only[file_company_item_only.index == i])\n\n#check NaN's again\nprint(\"\\n I've also removed some of the NaN's, but some remain\")\n#print(file.isna().sum(), len(file))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.469042Z","iopub.execute_input":"2023-08-25T15:27:04.469511Z","iopub.status.idle":"2023-08-25T15:27:04.521264Z","shell.execute_reply.started":"2023-08-25T15:27:04.469467Z","shell.execute_reply":"2023-08-25T15:27:04.520395Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now to the big problem: inconsistent columns\nKey points:\n\n- We've removed duplicates, cleaned up the non-numeric-convertible values in all the columns (with one big exception...), and dropped rows of all NaN's\n\n- The rows of all NaN's frequently look like headers of sections, by the their Company/Item values, or are drinks. Understanding the organization of some sections could be useful.\n\nNow about those column names.... There is a huge problem with this file!","metadata":{}},{"cell_type":"code","source":"#Examination of the csv in excel also revealed a singular row with new labels -- \n#indicating a change in the identity of the columns(!!). Extract the row and examine:\npd.set_option('display.max_columns', None)\nfile_weird_row = file[file['Sugars\\n(g)'] == 'Protein (g)']\nprint(\"There is a row in the dataset of all column names, after the Company/Item columns: \\n\")\nprint(file_weird_row) \nprint('\\n Note the index of 661, will look around the strange index to see what probably happened:\\n')\nprint(file[(file.index > 655) & (file.index < 665)])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.523354Z","iopub.execute_input":"2023-08-25T15:27:04.523925Z","iopub.status.idle":"2023-08-25T15:27:04.54326Z","shell.execute_reply.started":"2023-08-25T15:27:04.52389Z","shell.execute_reply":"2023-08-25T15:27:04.542081Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confirming the problem: some companies don't give calories from fat data, but this shifts every following column in the dataset\nSomething is very wrong with this file...... not only does it appear that the columns shift, but remember than calories from fat roughly equals 9*grams of fat. \nHowever, see line 656 --> \"calories from fat\" column is 12, but the \"grams of fat\" is 8g -- this is impossible, and this repeates in all the visible columns of this slice (646--664). \nSimilarly, the protein colum is >350, which is obviously wrong. This high of a number, does, however, match a weight watchers points score, not a protein (g) value.\nThis indicates a shift of values after the calories from fat column, which appears missing in these rows, even though the old column names persist in the data frame. \n\nOne consequence of this shift is that all rows with this problem have NaN for the final weight watchers column, and the many rows of the protein column has an impossibly high values (>calories/4) because of the different scale of the weight watcher points.\n\nNote that although the label change apparently occurs at index 661, the problem clearly precedes this index as well!\n\nFinally, do note that the row with the new column labels does sitll have an item list -- the Wendy's kid's meal. This might shift the rows, although other items later in the list make sense ( such as small-medium-large following consistent calorie trends)","metadata":{}},{"cell_type":"code","source":"#Time to fix it!\n\n#I leave in the following code, commented out, as a note of what I originally did, but looking at the file as a whole\n# & in particular looking at the company-by-company entries, revelaed that the problem is based on what\n#columns are there by company (with the Calories from fat column being absent from Wendy', KFC, and Pizza Hut\n#data)\n\n''' OLD ATTEMPT to fix via index:\n#check for likely bad rows (see part of the logic from the prior cell):\n#first remove weird row and convert to float so that conditional statements can be made:\nfile = file[file['Sugars\\n(g)'] != 'Protein (g)']\nfor i in file_no_company_item.columns:\n    file[i] = file[i].astype('float')\n\n#next make the conditional statements, which slice the dataframe by each condition chosen:\nw_w_pnts_NaN = file[file['Weight Watchers\\nPnts'].isna()]  # the suspect columns have no values in this column\ntoo_much_fat_for_fat_calories = file[(file['Calories from\\nFat'].astype('float') / 8) < file['Total Fat\\n(g)']] # having Calories from fat > 8*fat(g) should be impossible! I chose 8 instead of 9 grams since I think there is some variability in the conversion. \ntoo_much_protein_for_total_calories = file[(file['Calories'] / 4) < file['Protein\\n(g)']]  # similarly, there cant be more protein than the total calories / 4\n\n#overlap each condition, to retrieve the indexes where all three impossible conditions are met:\nindex_list = []\nfor i in w_w_pnts_NaN.index:\n    if (i in too_much_fat_for_fat_calories.index) & (i in too_much_protein_for_total_calories.index):\n        index_list.append(i) \n\nprint(index_list)\n#the problem appears to start at/around index 529\n#but first check near index 529 to see if anything was missed:\nprint(file[(file.index > 515) & (file.index < 535)])\n#weight watchers column ends abruptly at 529\n#store critical index:\ncrit_ind = index_list[0]\n\n# now shift all values after index 529 in columns after \"calories from fat\" over one, leaving the calories from fat column empty (all NaN)\n#perhaps I do this the hard way, by breaking the dataframe into various pieces then putting them back together agin properly. (I'm new to datascience / python)\n\n#make the latter half of the datafram, with / after the bad column, the values shifted to the proper columns:\ncolumns_with_cal_fat_no_ww = file.columns[3:-1]\ncolumns_after_cal_fat = file.columns[4:]\nfile_second_half = file[file.index > (crit_ind - 1)][columns_with_cal_fat_no_ww]\n\n#rename columns\ncol_dict = {}\nfor ii,i in enumerate(columns_with_cal_fat_no_ww):\n    col_dict[i] = columns_after_cal_fat[ii]\n\nfile_second_half = file_second_half.rename(columns = col_dict)\nfile_second_half['Calories from\\nFat'] = np.nan\n\n#take the first three (unaffected columns) of second half, then join it to the value columns:\nfile_first_three_columns_second_half = file[file.index > (crit_ind - 1)][file.columns[:3]]\nfile_second_half = pd.concat([file_second_half,file_first_three_columns_second_half], axis = 1)\n\n#no need to change the first half of the data frame\nfile_first_half = file[file.index < crit_ind]\n\n#finally join the two halves\nfile_reforged = pd.concat([file_first_half,file_second_half], axis = 0)\n\nprint(file_reforged)\n'''\n\n#New plan\n#first remove weird row and convert to float for the future & so I can do description of the stats of the columns:\n#I also got a strange result once from my later re-coding of the columns when I did not do this first, not sure how or if this is necessary\nfile = file[file['Sugars\\n(g)'] != 'Protein (g)']\nfor i in file_no_company_item.columns:\n    file[i] = file[i].astype('float')\n    \n#print(file.describe())\nprint('Look at which companies are affected by the column switch by looking for impossible values (like Cal-from-Fat / Total Fat << 9. or protein grams > about 100) and for all NaNs in the weightWatcher column \\n')\ncompanies = file['Company'].unique()\nfor i in companies:\n    print(i, \"--\", file[file['Company'] == i][['Calories from\\nFat','Total Fat\\n(g)','Protein\\n(g)','Weight Watchers\\nPnts']].head(n=2))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.544641Z","iopub.execute_input":"2023-08-25T15:27:04.544978Z","iopub.status.idle":"2023-08-25T15:27:04.591961Z","shell.execute_reply.started":"2023-08-25T15:27:04.544946Z","shell.execute_reply":"2023-08-25T15:27:04.59072Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fixing the column values of the companies with shifted/incorrect data (KFC,Wendy's,Pizza Hut)\nSo the problem appears to be what each company chooses to report, with some leaving out calories from fat. Specifically, Wendy's KFC and pizza hut need their columns' values shifted (starting with the calories from fat column on), while the others can be left the same.","metadata":{}},{"cell_type":"code","source":"#Separate the improply ordered data and the properly ordered, by company:\ncorrect_data_companies = [companies[0],\"Burger King\",\"Taco Bell\"]\nincorrect_data_companies = [companies[2],\"KFC\",\"Pizza Hut\"]\n\nslice_bad = file\nfor i in correct_data_companies:\n    slice_bad = slice_bad[slice_bad['Company'] != i]\n\nslice_good = file\nfor i in incorrect_data_companies:\n    slice_good = slice_good[slice_good['Company'] != i]\n\n# rename columns for the wrong half:\ncolumns_with_cal_fat_no_ww = file.columns[3:-1]\ncolumns_after_cal_fat = file.columns[4:]\ncol_dict = {\"Weight Watchers\\nPnts\":\"Calories from\\nFat\"}\nfor ii,i in enumerate(columns_with_cal_fat_no_ww):\n    col_dict[i] = columns_after_cal_fat[ii]\nslice_bad = slice_bad.rename(columns = col_dict)\n\n#Finally rejoin and check the outcome\nfile_reforged = pd.concat([slice_good,slice_bad], axis = 0)\nfile_reforged.sort_index()\n\nprint('Look at descriptive stats to see if problem appears fixed (esp. in the max column, look for impossible-sounding numbers like hundreds of grams of protein, that might indicate unfixed rows). This also serves as a first look at the statistics of the data now that the numeric columns are properly constructed and their values have been converted to a numeric type. \\n')\nfile_reforged.describe()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T15:27:04.593217Z","iopub.execute_input":"2023-08-25T15:27:04.593542Z","iopub.status.idle":"2023-08-25T15:27:04.656436Z","shell.execute_reply.started":"2023-08-25T15:27:04.593515Z","shell.execute_reply":"2023-08-25T15:27:04.65528Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Addressing the remaining Null values in the DataFrame\nLooks better! Although one has to wonder what item has >264g of sugar(!!)....\nNow I will drop all the NaN's (there are relatively few, less than 5% of the rows in the dataframe) except the NaN's in the Weight Watchers Points column, which I'll leave as is. \n\nNot only is the Weight Watchers column, from what I have seen, exclusively NaN for some companies (so I didn't want to impute a mean WW Points to those), but as we'll see later, the WW Points column does not convey much useful information beyond the information already captured by the Total Calories column of the dataframe (the two values correlate at 0.99!). ","metadata":{}},{"cell_type":"code","source":"# now that the dataframe has its substantial column issues fixed, it is time to look at those NaN's and deal with them\n#print(file_reforged.isna().sum(), len(file_reforged))\n\n# first, I will drop all the rows with NaNs below 5% of the columns (which is ~56, from 0.05*1126)\n#then fill all the NaN's of the calories from fat with 9*total fat\n# finally I will choose to bot impute the NaN's in the Weight Watchers column, as three of the companies simply have no data for that\ncols_to_drop_NaN = ['Total Fat\\n(g)','Saturated Fat\\n(g)','Trans Fat\\n(g)','Carbs\\n(g)','Fiber\\n(g)','Protein\\n(g)']\nfile_reforged = file_reforged.dropna(subset=cols_to_drop_NaN)\nfile_reforged['Calories from\\nFat'] = file_reforged['Calories from\\nFat'].fillna(9 * file_reforged['Total Fat\\n(g)'])\n\n#print(file_reforged.isna().sum(), len(file_reforged))  \nprint('Have dropped all nulls except those in the Weight Watcher Points column: \\n')\nprint(file_reforged.isna().sum())\n# I can see the changes have taken effect -- now it time to finally plot something / move past data cleaning!","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:04.657998Z","iopub.execute_input":"2023-08-25T15:27:04.658512Z","iopub.status.idle":"2023-08-25T15:27:04.674196Z","shell.execute_reply.started":"2023-08-25T15:27:04.658475Z","shell.execute_reply":"2023-08-25T15:27:04.673035Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Look at data: distribution of numerical variables","metadata":{}},{"cell_type":"code","source":"#time to look at the data for things of interest!\n#But first, let's look at outliers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Distributions of all the numerical data in the dataset\n\nfig,ax = plt.subplots(nrows = 3,ncols = 4,sharey=True)\ncol = 2\nfor axs in ax.flat:\n    axs.hist(file_reforged.iloc[:,col])\n    axs.set_title(file_reforged.columns[col].replace('\\n',' ').replace('Weight Watchers','WW'),size = 9,y = 0.7)\n    axs.set_ylim(top = 1000)\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset\")\nplt.show()\n\n#boxplot\nfig,ax = plt.subplots(nrows = 3,ncols = 4,sharey=True)\ncol = 2\nfor axs in ax.flat:\n    axs.boxplot(file_reforged.iloc[:,col])\n    axs.set_title(file_reforged.columns[col].replace('\\n',' ').replace('Weight Watchers','WW'),size = 9,y = 1.0)\n    axs.set_ylim(top = 3000)\n    axs.set_xticklabels('')\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:04.675746Z","iopub.execute_input":"2023-08-25T15:27:04.678033Z","iopub.status.idle":"2023-08-25T15:27:07.789323Z","shell.execute_reply.started":"2023-08-25T15:27:04.677955Z","shell.execute_reply":"2023-08-25T15:27:07.788298Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Potentially useful Transformation: cubed-root\n\nWith the except of the total calories and WW pnts columns, the distributions visible above are very right-skewed and have lots of values piled up near 0 with large tails of higher-value outliers, so a transformation of the data may be useful. \n\nI tried a log-transformation first, but settled on a root transformation because the many zero values in the dataset (even with the addition of a small number like 0.0001 to avoid infinities) seemed to throw off the output for some of the downstream analysis I was doing. A cubed-root ended up being simpler to execute and appeared to convert the data to relatively normal distributions for most columns, except for the pile of values remaining at 0 in many of them:","metadata":{}},{"cell_type":"code","source":"#Most of these are heavily right-skewed distributions, so let's transform them by taking the cubed root and see what happens:\nfile_re_trans = pd.DataFrame()\nfor i in file_no_company_item.columns:\n    file_re_trans[i] = (file_reforged[i]) ** (1/3)\n\n#Distributions of all the numerical data in the transformed dataset\n\nfig,ax = plt.subplots(nrows = 3,ncols = 4,sharey=True)\ncol = 0\nfor axs in ax.flat:\n    axs.hist(file_re_trans.iloc[:,col])\n    axs.set_title(file_re_trans.columns[col].replace('\\n',' ').replace('Weight Watchers','WW'),size = 9,y = 0.7)\n    axs.set_ylim(top = 1000)\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset: cubed-root transformation\")\nplt.show()\n\n#boxplot\nfig,ax = plt.subplots(nrows = 3,ncols = 4,sharey=True)\ncol = 0\nfor axs in ax.flat:\n    axs.boxplot(file_re_trans.iloc[:,col])\n    axs.set_title(file_re_trans.columns[col].replace('\\n',' ').replace('Weight Watchers','WW'),size = 9,y = 1.0)\n    axs.set_ylim(top = (3000 **(1/3)))\n    axs.set_xticklabels('')\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset: cubed-root transformation\")\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:07.790594Z","iopub.execute_input":"2023-08-25T15:27:07.790937Z","iopub.status.idle":"2023-08-25T15:27:10.178965Z","shell.execute_reply.started":"2023-08-25T15:27:07.790908Z","shell.execute_reply":"2023-08-25T15:27:10.177772Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributions of calories, sodium, and fiber by company","metadata":{}},{"cell_type":"code","source":"#Much better! If called for, we can always use a cubed-root transformation, but the transformed dataframe currently doesn't have the first two columns: \nfile_re_trans['Company'] = file_reforged['Company']\nfile_re_trans['Item'] = file_reforged['Item']\n\n#Although there are still some outliers in the data, I want to move on to visualization and analysis for now \n\n#The question is -- what do we what to know?\n#I see other have split things up by company, so I'll look at that too\nfile_reforged['Company'].value_counts().plot(kind='bar')\nplt.title(\"Items in the Dataset from each Company\")\nplt.show()\n\nsns.catplot(kind='box',data=file_reforged,x='Company',y='Calories')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Calories by item from each company\")\nplt.show()\n\nsns.catplot(kind='box',data=file_reforged,x='Company',y='Sodium \\n(mg)')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Sodium by item from each company\")\nplt.show()\n\nsns.catplot(kind='box',data=file_reforged,x='Company',y='Fiber\\n(g)')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Fiber by item from each company\")\nplt.show()\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:10.182769Z","iopub.execute_input":"2023-08-25T15:27:10.183995Z","iopub.status.idle":"2023-08-25T15:27:11.659434Z","shell.execute_reply.started":"2023-08-25T15:27:10.183952Z","shell.execute_reply":"2023-08-25T15:27:11.658292Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Optional: print heatmaps of the correlations between columns at different calorie levels (<50,50-150,150-450,>450 total calories)\n","metadata":{}},{"cell_type":"code","source":"''' \n#but I also want to know about how many items are of different levels of calories, like sauces with very low calories, perhaps. I might use these later.\nxlow_cal = file_reforged[file_reforged['Calories'] <= 50]\n\n#could be good to exclude very low calorie items:\n#cal_more_than_50 = file_reforged[file_reforged['Calories'] > 50]\n\nlow_cal = file_reforged[(file_reforged['Calories'] <= 150)  & (file_reforged['Calories'] > 50)]\nmed_cal = file_reforged[(file_reforged['Calories'] <= 450)  & (file_reforged['Calories'] > 150)]\nhigh_cal = file_reforged[file_reforged['Calories'] > 450]\nlisty = [xlow_cal,low_cal,med_cal,high_cal]\ntitle_list=['<50','50-150','150-450','>450']\n#print(low_cal,len(low_cal))\nfor ii,i in enumerate(listy):\n    corr = i.corr()\n    sns.heatmap(corr,annot=True,linewidth=0.5,fmt = '0.2f')\n    plt.title('Correlations of {} calorie items'.format(title_list[ii]))\n    plt.show()\n'''\nprint('Uncomment code, or run directly by highlight --> ctrl+shift+enter for a lot of heatmaps across the various categories of calorie content')","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:11.660927Z","iopub.execute_input":"2023-08-25T15:27:11.661252Z","iopub.status.idle":"2023-08-25T15:27:11.667045Z","shell.execute_reply.started":"2023-08-25T15:27:11.661223Z","shell.execute_reply":"2023-08-25T15:27:11.665876Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation visualizations\n\nLet's see how the columns correlate with each other. This will also be useful for any downstream machine learning analysis, as we will be able to drop redundant columns.","metadata":{}},{"cell_type":"code","source":"\n#Let's see if we can predict company from the characteristics of the items:\n#First let's find highly correlated data and drop redundant/correlated columns\ncorr = file_reforged.corr()\nsns.heatmap(corr,annot=True,linewidth=0.5,fmt = '0.2f')\nplt.title('Correlations of untransformed data, all columns')\nplt.show()\n\n\n#almost all of the columns values are positively correlated (if weakly) with each other, except sugar\n\n#some correlation comparisons\n\n#Is the weight Watchers column just a proxy for total calories? will drop NaN rows\nfile_WW_comp = file_reforged.dropna(subset = 'Weight Watchers\\nPnts')\n\nsns.lmplot(data=file_WW_comp, x = 'Weight Watchers\\nPnts',y = 'Calories',line_kws={'color':'r'})\nplt.title('Calories vs. Weight Watcher Points')\nplt.show()\n\n#Fat vs. Sugar\nsns.lmplot(data=file_reforged, x = 'Sugars\\n(g)',y = 'Total Fat\\n(g)',line_kws={'color':'r'})\nplt.title('Grams of Fat vs. Sugar')\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:11.668572Z","iopub.execute_input":"2023-08-25T15:27:11.668947Z","iopub.status.idle":"2023-08-25T15:27:13.417767Z","shell.execute_reply.started":"2023-08-25T15:27:11.66891Z","shell.execute_reply":"2023-08-25T15:27:13.416364Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion of correlation visualizations:\nWe can see that most measured values in the dataset correlate with each other positively, except sugar.\nInterestingly, when we look at the scatter plot of sugar vs. fat quantities, we can see that the negative correlation is driven by a large number of items that are either predominantly fat with no sugar, or vice versa, seen by the points tightly hugging the x and y axes in the final figure of this section.\n\nAnother point of interest is that the \"Weight Watchers Points\" is extremely tightly correlated with total calories (correlation = 0.99), perhaps indicating that the Weight Watcher Points system is not contributing additional information about fast food items beyond total calorie count.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Can we predict the company from the item characteristics?\n\nPerhaps we can use machine learning to train an algorithm to identify the company name from the nutrient characteristics of the item.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n\n\n#preparefile\n\n#print(file_re_trans.columns)  -- remembering the column names this deep in the notebook is easier if you print them...\n\n#all the fat columns & Cholesterol are highly correlated, drop all except Total Fat\nfile_drop = file_re_trans.drop(['Saturated Fat\\n(g)','Calories from\\nFat','Cholesterol\\n(mg)'],axis = 1)\n#Weight Watcher points correlates extremely closely with total calories, also was imputed for half of the companies in the dataset,: also drop\nfile_drop = file_drop.drop(['Weight Watchers\\nPnts'],axis = 1)\n\n#let's do this only for the items > 50 Calories:\nfile_drop = file_drop[file_drop['Calories'] > (50**(1/3))]\n\nSEED = 656\n\nX = file_drop.drop(['Item','Company'],axis = 1)\n\ny = file_drop['Company']\ny = y.factorize()\ny = y[0]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = SEED)\n\n#let's see if we can identify companies from their items\nsc = StandardScaler()\nxg = xgb.XGBClassifier(objective = \"reg:linear\",random_state=SEED)\nparam = {\"max_depth\":[10,15,20],'n_estimators':[10,20],'gamma':[0.05,0.15,3]}\ncv = GridSearchCV(xg,param_grid=param,cv = 5)\n\nsc.fit_transform(X_train)\nsc.transform(X_test)\nmodel = cv.fit(X_train,y_train)\nbest = model.best_estimator_\nbest_param = model.best_params_\nscore = best.score(X_test,y_test)\nprint('For this model, I dropped items with total calories <50, dropped a number of redundant columns, and used the cubed-root transformed data. \\n')\nprint('Best Scoring Model Accuracy:\\n', score, '\\n\\n', 'Best Tested Parameters of the XGBoost model:', best_param)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T15:27:13.419295Z","iopub.execute_input":"2023-08-25T15:27:13.419739Z","iopub.status.idle":"2023-08-25T15:27:27.84638Z","shell.execute_reply.started":"2023-08-25T15:27:13.419696Z","shell.execute_reply":"2023-08-25T15:27:27.845416Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not very good at predicting the company, with only ~64% accuracy! \nIf I included items of <50 calories that accuracy score went down to 58%.\n","metadata":{}}]}