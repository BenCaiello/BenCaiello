{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bencaiello/top-1000-yt-ers-w-language-translation-english?scriptVersionId=143687055\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\n\n# Install translation package (only need to do once!)\ntry:\n    import translators as ts\nexcept:\n    !pip install translators\n    import translators as ts\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filepath = (os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-20T21:31:30.203229Z","iopub.execute_input":"2023-09-20T21:31:30.204386Z","iopub.status.idle":"2023-09-20T21:31:30.234092Z","shell.execute_reply.started":"2023-09-20T21:31:30.204311Z","shell.execute_reply":"2023-09-20T21:31:30.23312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that this first step may run slowly if you copy this notebook, as a download from pip is used for the translation package. This only should occur on the first run of the notebook.","metadata":{}},{"cell_type":"markdown","source":"# Introduction & First Look at Data!\n\nYoutube is a major social media platform dedicated to allowing users to publish, share, and watch video content. This data set contains data about the top 1000 users of Youtubes -- the top 1000 channels -- including the number of subscribers to each channel, as well as the average engagement per video (visits/views, likes, comments) that a channel gets. \n\nAdditionally, many (but not all) channels have a known country of origin and/or category of content that they produce.\n\nIn this notebook, we will plot relationships primarily about total subscriber count and how it relates to the country / category of content produced by a given top 1000 youtube channel, but also some other interesting questions -- like how do subscribers counts relate to engagement per video? & Do 20% of the top 1000 channels have 80% of the subscribers?\n\nThis first step I will display some of the data anda information about the uncleaned dataframe, and then there is some data cleaning / rearranging going on insde the hidden code block to address some of the issues I found. It don't don't go into the details of that in this visible text, but if you are interested, go look inside the hidden code block to see what and why I did certain steps!","metadata":{}},{"cell_type":"code","source":"file = pd.read_csv(filepath)\n\n# Inspect the nature of the data in each column, the number of null values and the datatypes of the columns\ndisplay(file.head())\ndisplay(file.info())\n\n# Check duplicates. Note that simply checking duplicates on the overall dataframe will fail because the rank of every entry is unique.\nduplicate_users = file[file['Username'].duplicated()]\n\n# Remove triple quotes to see the duplicated entries:\n'''\nfor i in duplicate_users['Username']:\n    print(file[file['Username'] == i])\n'''\n\n# Drop all duplicated entries. Note some have varied numerical values. Just dropped the second occurance of each duplicate.\nfile = file.drop(duplicate_users.index)\nfile = file.reset_index()\nfile = file.drop('index', axis = 1)\n\n#  The numerical columns do not need to be floats as they represent discrete units. \n# Note: trying to read these columns (3,5-7) directly as 'int' type in the pd.read_csv call did not succeed.\nfor i in file.columns:\n    try:\n        file[i] = file[i].astype('int')\n    except:\n        pass\n\n# The Country column, although having no null values has 'Unknown' values\n# Will convert the nulls in categories to 'Unknown' for consistency.\n# Do not want to drop nulls because there are too many in the categories column. Instead Unknown will be its own category.\nfile['Categories'] = file['Categories'].fillna('Unknown')\n\n# Additionally, I will drop the links column, as it does not provide useful information for most visualizations.\nfile = file.drop('Links',axis = 1)\n\n# I will make a copy of the dataframe for use later (under the correlation header)\nfresh_file = file.copy()\n\n# I will also create a column of subscribers in millions (easier to plot)\nfile['Subscribers (millions)'] = file['Suscribers'] / 1000000","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-20T21:31:30.236413Z","iopub.execute_input":"2023-09-20T21:31:30.236866Z","iopub.status.idle":"2023-09-20T21:31:30.296822Z","shell.execute_reply.started":"2023-09-20T21:31:30.236826Z","shell.execute_reply":"2023-09-20T21:31:30.295493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Translation of Category and Country columns to English\n\nI want all the country and category names to be in English, but along the way, I also convert the category column into a column of lists (instead of a column of comma separated strings). This change may be useful later when dealing with the fact that each channel can have more than one category!","metadata":{}},{"cell_type":"code","source":"# I am a monolingual English speaker -- so let's convert the country / category names into English!\n# Link to package:  https://pypi.org/project/translate-api/\n# Note that documentation at the provided link is inaccurate!\n\n# First, identify the categories in the Categories columns. Note that some channels are more than one category.\ncat_list = []\nfor i in file['Categories']:\n    split_list = []\n    split_list = i.split(',')\n    for j in split_list:\n        j = j.strip()\n        cat_list.append(j)\n\n# Now I isolate the unique categories, by using the set datatype:\ncat_set = set(cat_list)\n\n\n# Next, I convert the unique categories into a single string of each category, separated by commas.\n# This is important to reduce the number of queries to the translation tool / URL to a minimum.\n# The translate_text function throws an error if you query the same URL more than ~7 times in rapid succession.\ncat_list = list(cat_set)\ncat_str = ''\nfor i in cat_list:\n    cat_str = cat_str + i + ','\n    \n# Now we translate:\nenglish = ts.translate_text(cat_str)\n\n# Then undo the single string back into a list:\nenglish_list = english.split(',')\n\n# Then make a dictionary to match the Spanish phrases to the English translations.\ntrans_dict = {}\nfor i,ii in enumerate(cat_list):\n    trans_dict[ii] = english_list[i]\n    \n# Next we translate the column using the dictionary:\ntranslation_list = []\nfor i in file['Categories']:\n    entry = i.split(',')\n    entry_list = []\n    for j in entry:\n        j = j.strip()\n        j = trans_dict[j]\n        entry_list.append(j)\n    translation_list.append(entry_list)\nfile['Categories'] = translation_list\n\n# Countries are much simpler, as they only have singular values per entry:\n# A similar process is followed as above, with fewer steps. \n# Consult the comments above if you want to understand why each step is taken\ncountry_list = list(file['Country'].unique())\ncountry_str = ''\nfor i in country_list:\n    country_str = country_str + i + ','\nenglish_c = ts.translate_text(country_str)\nenglish_list_c = english_c.split(',')\ntrans_dict = {}\nfor i,ii in enumerate(country_list):\n    trans_dict[ii] = english_list_c[i]\nfile['Country'] = file['Country'].replace(trans_dict)\n\ndisplay(file.head())","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-20T21:31:30.300031Z","iopub.execute_input":"2023-09-20T21:31:30.300838Z","iopub.status.idle":"2023-09-20T21:31:31.520391Z","shell.execute_reply.started":"2023-09-20T21:31:30.300795Z","shell.execute_reply":"2023-09-20T21:31:31.519542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make dummy variables for every category & begin to plot!\n\nI add columns for each of the ~24 categories, with each channel receiving a value of 0 if they aren't in that category or 1 if they are. \n\nThis is useful for plotting the categories later, and for containing all the information in the category column, including channels with more than one category, while also not trying to change the shape of the dataframe.\n\nNext I begin plotting the distribution of the numerical variables in the dataset, to see if they are normally distributed and whether a logarithmic transformation of the data might be useful when plotting / analyzing the data.","metadata":{}},{"cell_type":"code","source":"# Make dummy variable columns!\nenglish_list = english_list[0:-1]\nfor i in english_list:\n    T_F_list = []\n    for j in file['Categories']:\n        if i in j:\n            T_F_list.append(1)\n        else:\n            T_F_list.append(0)\n    file[i] = T_F_list\ndisplay(file.head())\n\n# change likes / comments in main dataframe so they can be plotted as log-values:\nfile['Likes'] = (file['Likes'] + 0.01) / 10000\nfile['Comments'] = (file['Comments'] + 0.001) / 1000\nfile['Visits'] = (file['Visits'] + 0.001) / 10000\n\n# Look at distribution of numerical variables:\ndist_list = ['Subscribers (millions)','Likes','Comments','Visits']\ndist_df = file[dist_list]\n\nsns.boxplot(dist_df)\nplt.title('Distribution of Subs (in millions), Likes (1000s), Comments (1000s), & Visits (1000s)')\nplt.show()\n\nprint('\\n The data is extremely skewed! \\n')\n\nsns.boxplot(dist_df)\nplt.title('Distribution of Subs (in millions), Likes (1000s), Comments (1000s), & Visits (1000s) on Log scale')\nplt.yscale('log')\nplt.show()\n\nprint('\\n Will use log distribution when plotting likes/comments! \\n')\n\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-20T21:31:31.521916Z","iopub.execute_input":"2023-09-20T21:31:31.522512Z","iopub.status.idle":"2023-09-20T21:31:32.386658Z","shell.execute_reply.started":"2023-09-20T21:31:31.52248Z","shell.execute_reply":"2023-09-20T21:31:32.385557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categories: Counts and AVG Subscribers by category\n\nNow I will plot the number of channels in each category, the total number of subscribers represented in each category, and the average number of subscribers per channel in each category.\n\nNote that since each channel can have more than one category, the numbers of subscribers from all the categories added up should be greater than the overall number of subscribers of the channels of the dataset.\n\nAdditionally, the total number of subscribers is >20,000 million (or in other words, >20 billion). Since the total world population is around 8 billion, this indicates a lot of overlapping subscribers (people subscribed to more than one of the top 1000 channels), duplicate accounts, and/or fake accounts inflating the numbers seen here! Likely a combination of all of these, especially the ","metadata":{}},{"cell_type":"code","source":"# Make a data frame with all the categories of channel\n# Note that this has to be a new dataframe, as it will bel onger than 1000\n\ncat_counts = pd.DataFrame()\ncat_list = []\nfor i in file['Categories']:\n    for j in i:\n        cat_list.append(j)       \ncat_counts['Categories'] = cat_list\ncat_count_ordered_list = cat_counts['Categories'].value_counts().index\n\nsns.countplot(x=cat_counts['Categories'], order = cat_count_ordered_list)\nplt.xticks(rotation=90,size = 10)\nplt.title('Channel Counts of Each Category')\nplt.xlabel(None)\nplt.show()\n\n\nloop_dict = {}\ni = 10\nwhile i < 33:\n    loop_list = file[file[file.columns[i]] == 1]['Subscribers (millions)']\n    loop_dict[file.columns[i]] = loop_list\n    i += 1\nloop_dict['Overall'] = file['Subscribers (millions)']\n\ncats = pd.DataFrame(loop_dict)\ncat_mean_order = cats.mean().sort_values(ascending = False).index\ncat_total_order = cats.sum().sort_values(ascending = False)\nov_mean = loop_dict['Overall'].mean()\n\nplt.bar(cat_total_order.index, cat_total_order)\nplt.xticks(rotation = 90, size = 10)\nplt.title('Total Subscribers from all Channels for a given Category')\nplt.ylabel('Total Subscribers')\nplt.show()\n\n\nsns.barplot(cats, order = list(cat_mean_order),color = 'r')\nplt.xticks(rotation = 90, size = 10)\nplt.title('Average Subscribers per Channel by Category')\nplt.ylabel('AVG Subscribers (Millions)')\nplt.hlines(ov_mean,xmin = -1, xmax = 23, linestyles = 'dashed' )\nplt.annotate('Avg subs',xy=(20,ov_mean + 1),size = 10)\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-20T21:31:32.390175Z","iopub.execute_input":"2023-09-20T21:31:32.390634Z","iopub.status.idle":"2023-09-20T21:31:36.436438Z","shell.execute_reply.started":"2023-09-20T21:31:32.390595Z","shell.execute_reply":"2023-09-20T21:31:36.435162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Only four categories outperform the overall average: Toys, Music & Dance, Education, Video Games, and Animation!**","metadata":{}},{"cell_type":"markdown","source":"# Now by Country!\n\nHere I do a very similar analysis as what I did for the visualization by category above, just here by country. Note that since there are no channels with more than one country, the total from each country ought to add up to the overall number of subscribers across the 1000 channels.","metadata":{}},{"cell_type":"code","source":"country_counts = file['Country'].value_counts().index\n\nsns.countplot(x=file['Country'], order = country_counts)\nplt.xticks(rotation=90, size = 10)\nplt.title('Channel Counts of Each Country')\nplt.xlabel(None)\nplt.show()\n\ntotal_subs_per_country = file.groupby('Country')['Subscribers (millions)'].sum().sort_values(ascending = False)\n\nplt.bar(x = total_subs_per_country.index, height = total_subs_per_country)\nplt.xticks(rotation = 90, size = 10)\nplt.title('Total Subscribers from all Channels in a Given Country')\nplt.ylabel('Total Subscribers (millions)')\nplt.show()\n\navg_subs_per_country_list = file.groupby('Country')['Subscribers (millions)'].mean().sort_values(ascending = False).index\n\nsns.barplot(file, x = file['Country'], y = 'Subscribers (millions)', order = list(avg_subs_per_country_list),color = 'r')\nplt.xticks(rotation = 90, size = 10)\nplt.title('Average Subscribers per Channel by Category')\nplt.ylabel('AVG Subscribers (Millions)')\nplt.hlines(ov_mean,xmin = -1, xmax = 28, linestyles = 'dashed' )\nplt.annotate('Avg subs',xy=(22,ov_mean + 1),size = 10)\nplt.show()\n\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-20T21:31:36.43812Z","iopub.execute_input":"2023-09-20T21:31:36.439469Z","iopub.status.idle":"2023-09-20T21:31:39.404167Z","shell.execute_reply.started":"2023-09-20T21:31:36.439408Z","shell.execute_reply":"2023-09-20T21:31:39.402607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Once again, only a few countries exceed the average subs / channel! Likely the high number of India and 'Unknown' country channels and their relatively higher subscribed count are pulling up the overall average.**","metadata":{}},{"cell_type":"markdown","source":"# Test the Pareto Distribution! Do 20% of the channels have 80% of the Subscribers?\n\n\nThe Pareto distribution is a principle about how the top few percent possess or produce the majority of a given resource -- perhaps in this case, something like YT subscribers?\n\nOne name for this is the \"80-20\" rule, aka that 80 of the given resource (the principle was originally discovered in terms of wealth) is held by only 20% people. \n\n[Follow this link to see a discussion of this principle, & as a source!](https://dlab.berkeley.edu/news/explaining-80-20-rule-pareto-distribution#:~:text=The%20Pareto%20distribution%20is%20a%20power%2Dlaw%20probability%20distribution%2C%20and,sloped%20(see%20Figure%201).)\nYou will also see at this link that the general shape of the Pareto distribution -- high values at the start that rapidly drop off --  has showed up throughout the earlier visualizations of subscriber count across categories / countries. Her though, I compare to the subsrcribers of the channels, not the categories/countries.","metadata":{}},{"cell_type":"code","source":"# Recall that 6 duplicate entries were dropped, so instead of 1000 [999] index, and 200 [199] index for the total\n# length of the dataframe & the 20% mark, respectively, I use [993] and [198].\n\nplt.bar(x=file.index,height=file['Subscribers (millions)'],edgecolor = 'b',color = 'b')\nplt.vlines(198, ymin = 0, ymax = 250,color = 'r')\nplt.annotate('Top 20%',xy = (10,250), size = 10)\nplt.annotate('Lower 80%',xy = (250,250), size = 10)\nplt.title('Distribution of Subs in order of rank')\nplt.ylabel('Subscribers (millions)')\nplt.xlabel('Rank')\nplt.show()\n\nfile['Subscribers cumulative (mil)'] = file['Subscribers (millions)'].cumsum()\n\nsns.displot(y = file['Subscribers cumulative (mil)'], kind = 'ecdf')\nplt.vlines(0.198, ymin = 0, ymax = file['Subscribers cumulative (mil)'][993],color = 'r')\nplt.annotate('Top 20%',xy = (0,21000), size = 10)\nplt.annotate('Lower 80%',xy = (0.21,21000), size = 10)\nplt.xlabel('Proportion of Channels included in Cumulative Subscribers')\nplt.title('Cumulative Subscribers over top 1000 YT channels')\nplt.show()\n\ntwenty = file['Subscribers cumulative (mil)'][198]\ntotal = file['Subscribers cumulative (mil)'][993]\neighty = total - twenty\n\nprint('Number of Subscribers from top 200 channels (in millions): ', round(twenty, 3))\nprint('% of total: ', round(twenty / total, 3) * 100)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-20T21:31:39.405986Z","iopub.execute_input":"2023-09-20T21:31:39.406528Z","iopub.status.idle":"2023-09-20T21:31:42.074493Z","shell.execute_reply.started":"2023-09-20T21:31:39.40649Z","shell.execute_reply":"2023-09-20T21:31:42.072809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer:**\n\nNo! here 20% of the top 1000 YT channels have 40% of their subscribers, not 80%!","metadata":{}},{"cell_type":"markdown","source":"# What about per video engagement? Does is relate to Subscriber count?\n\nLet's look at how engagement per video tracks with total subscribers!\n\nHowever, a number of channels have 0 average visits, likes, and/or comments per video. While this might be possible for comments (some videos disable comments), it seems improbable that any of the top 1000 channels would have zero views / video or zero likes / video. \nI will drop all rows where there are zero visits as these channels all also had 0 likes / comments. This still leaves the rows with non-zero visits / comments but zero likes. For now I will keep them, and they only represent a small number (28) of the entries in the dataset.\n\nNote that subscribers are in millions, while the others are in units of 1,000s before the log transformation.","metadata":{}},{"cell_type":"code","source":"# Plot these columns after log transformation\nfile['log Likes'] = np.log(file['Likes'])\nfile['log Comments'] = np.log(file['Comments'])\nfile['log Subs'] = np.log(file['Subscribers (millions)'])\nfile['log Visits'] = np.log(file['Visits'])\n\n# Recall I made a copy of the file ('fresh_file') after the initial cleaning steps. \n# Here I use it to slice the transformed dataframe, as the transformed dataframe does not have 0 values for the log-transformed columns:\nzero_visits = file[fresh_file['Visits'] == 0]\ndisplay(zero_visits)\n\n# As it turns out all channels with 0 average views also have 0 likes / comments.\n# This looks like a data scraping / collection problem (perhaps?), so I will drop all these rows when doing correlations:\nfile = file.drop(zero_visits.index)\nfresh_file_no_zero = fresh_file.drop(zero_visits.index)\n\nzero_likes = file[fresh_file_no_zero['Likes'] == 0]\nzero_likes.shape","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-20T21:31:42.076123Z","iopub.execute_input":"2023-09-20T21:31:42.076507Z","iopub.status.idle":"2023-09-20T21:31:42.146919Z","shell.execute_reply.started":"2023-09-20T21:31:42.076475Z","shell.execute_reply":"2023-09-20T21:31:42.145741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = np.corrcoef(file['log Likes'],file['log Subs'])\ncorrelation = 'corr = ' + str(round(corr[0][1],2))\n\n\nsns.regplot(file,x='log Subs',y = 'log Likes', ci = None, line_kws = {'color':'r', 'linestyle':'dashed'})\nplt.title('Likes per video vs. Subscribers')\nplt.annotate(correlation, xy = (5,0.1), size = 10)\nplt.show()\n\ncorr = np.corrcoef(file['log Comments'],file['log Subs'])\ncorrelation = 'corr = ' + str(round(corr[0][1],2))\n\nsns.regplot(file,x='log Subs',y = 'log Comments', ci = None, line_kws = {'color':'r', 'linestyle':'dashed'})\nplt.title('Comments per video vs. Subscribers')\nplt.annotate(correlation, xy = (4.5,-7.5), size = 10)\nplt.show()\n\ncorr = np.corrcoef(file['log Visits'],file['log Subs'])\ncorrelation = 'corr = ' + str(round(corr[0][1],2))\n\nsns.regplot(file,x='log Subs',y = 'log Visits', ci = None, line_kws = {'color':'r', 'linestyle':'dashed'})\nplt.title('Visits per video vs. Subscribers')\nplt.annotate(correlation, xy = (4.75,2), size = 10)\nplt.show()\n\n\ncorr = np.corrcoef(file['log Comments'],file['log Likes'])\ncorrelation = 'corr = ' + str(round(corr[0][1],2))\n\nsns.regplot(file,x='log Likes',y = 'log Comments', ci = None, line_kws = {'color':'r', 'linestyle':'dashed'})\nplt.title('Comments per video vs. Likes per video (logarithmic scale)')\nplt.annotate(correlation, xy = (-12,-10), size = 10)\nplt.show()\n\ncorr = np.corrcoef(file['log Visits'],file['log Likes'])\ncorrelation = 'corr = ' + str(round(corr[0][1],2))\n\nsns.regplot(file,x='log Likes',y = 'log Visits', ci = None, line_kws = {'color':'r', 'linestyle':'dashed'})\nplt.title('Visits per video vs. Likes per video')\nplt.annotate(correlation, xy = (-12,-3), size = 10)\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-20T21:31:42.148819Z","iopub.execute_input":"2023-09-20T21:31:42.149179Z","iopub.status.idle":"2023-09-20T21:31:44.411203Z","shell.execute_reply.started":"2023-09-20T21:31:42.14915Z","shell.execute_reply":"2023-09-20T21:31:44.409833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Per Video Likes and Visits / Comments correlate well with each other -- but these do not strongly correlate with subscribe count!**\n\nWhile it might be expected that the most successful channels (in terms of subscribers) would also be the most successful in terms of per video engagement, this does not seem to be the case. There could be a number of reasons for this discrepancy:\n\n-- There continues to be the possibility is that there is an issue with the underlying data, as the presence of top 1000 channels with an average of 0 likes per video seems fairly improbable. However, even apart from the few channels with 0 values for likes, there does not seem to be a strong trend between visits and subscribers nor sith the other two engagement metrics.\n\n-- For commments specifically, some channels may, by default, prohibit comments. This guarantees a 0 comment count regardless of other metrics of engagement or of video quality. \n\n-- Some channels may specialize in large volumes of videos, where high engagement per video is less important to the overall subscriber count. For example, a channel that specializes in many short sports clips might have limited engagement per video while still having many sports-enthusiast subscribers. If many sports are represented in the clips, than a given subscriber is likely to only engage with a few videos in the sport of their interest and not the rest of the channels videos.\n\nI'm interested in whether this lack of correlation between channels subscribers and per video engagement is true of all categories of video, or perhaps certain categories of video do show evidence of a relationship between these metrics.\nLet's look a bit deeper!\n","metadata":{}},{"cell_type":"markdown","source":"# Engagement and Correlations to Subscribers by category!\n\nMaybe subscribers and engagement don't correlated well in the overall dataset, but perhaps this an effect of video category?","metadata":{}},{"cell_type":"code","source":"# Planned!","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:31:44.413145Z","iopub.execute_input":"2023-09-20T21:31:44.414376Z","iopub.status.idle":"2023-09-20T21:31:44.420412Z","shell.execute_reply.started":"2023-09-20T21:31:44.414301Z","shell.execute_reply":"2023-09-20T21:31:44.418863Z"},"trusted":true},"execution_count":null,"outputs":[]}]}