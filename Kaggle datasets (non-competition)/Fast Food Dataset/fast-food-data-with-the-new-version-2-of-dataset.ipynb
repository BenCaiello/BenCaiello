{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bencaiello/fast-food-data-with-the-new-version-2-of-dataset?scriptVersionId=143137297\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# data wrangling imports:\nimport pandas as pd \nimport numpy as np\n\n# visualization imports:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning imports:\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n\n#Remove irritating warnings from output\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:27.81458Z","iopub.execute_input":"2023-09-15T18:03:27.816813Z","iopub.status.idle":"2023-09-15T18:03:27.831019Z","shell.execute_reply.started":"2023-09-15T18:03:27.816741Z","shell.execute_reply":"2023-09-15T18:03:27.829719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Look\nNow that the standard Kaggle import is done, let's open the file and look at it a bit\n\nNote that is it a modified/updated version of my \"Fast Food with heavy cleanup\" notebook, using the new filepath & removing some of the cleanup steps that the prior version of the dataset required. If you want to see what the problems with the older version of the dataset were & how I fixed them, you can go to that notebook and/or see the discussion topic about this for this dataset.","metadata":{}},{"cell_type":"code","source":"# load file\nfile = pd.read_csv('/kaggle/input/fast-food/FastFoodNutritionMenuV2.csv')\n\n# exploratory examination of file\ndisplay(file.head())\nfile.info()\n# file.isna().sum()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:27.833285Z","iopub.execute_input":"2023-09-15T18:03:27.833667Z","iopub.status.idle":"2023-09-15T18:03:27.90722Z","shell.execute_reply.started":"2023-09-15T18:03:27.833634Z","shell.execute_reply":"2023-09-15T18:03:27.905514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting columns to numeric: checking what doesn't fit\nAll columns except Company / Item should be numeric (I will use float type) but are currently object type, so we will need to eventually convert them to float.\n\nAt the same time, I also want to know what is in my columns that aren't numeric, so I wrote a function that will reveal what kinds of values cannot be converted to float in each of these columns, so that I can see what other cleaning steps might be used to salvage data / determine whether other errors may be present:","metadata":{}},{"cell_type":"code","source":"def df_non_numeric_list(dataframe, column_exclusion_list = None):\n    \"\"\" Print all unique values in a dataframe, by column, that cannot be converted to a float.\n    \n    Parameters\n    ----------\n    dataframe (pandas dataframe): required, the dataframe whose columns are to be analyzed for non-numeric convertible values\n    column_exclusion_list (list): optional, a list of column names of those columns to be excluded from the analysis. Default value is None.\n    \n    Returns\n    -------\n    Prints: the name of each column in the dataframe, followed by a list of all the non-numeric convertible values in the column.\n    \"\"\"\n    for column in dataframe.columns:\n        error_list = []\n        if column in column_exclusion_list:\n            pass\n        else:\n            for element in dataframe[column]:\n                try:\n                    element_float = float(element)\n                except:\n                    error_list.append(element)\n            print(column , ': ', pd.Series(error_list).unique())\n\n# Check what non-numerics are hiding in the columns that ought to be numeric....\ndf_non_numeric_list(file,['Company', 'Item'])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T18:03:27.909767Z","iopub.execute_input":"2023-09-15T18:03:27.910254Z","iopub.status.idle":"2023-09-15T18:03:27.932384Z","shell.execute_reply.started":"2023-09-15T18:03:27.910217Z","shell.execute_reply":"2023-09-15T18:03:27.930334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fixing non-numeric values in the numerical columns\nI found that the values in a non numeric-convertable form were either:\n  \n  \n  '\\xa0', \n \n \n '<1' or '<5', \n \n \n plus one instance where the units were included with a usable number ('5.5 g')","metadata":{}},{"cell_type":"code","source":"# Simple cleaning\nfile = file.replace(['\\xa0', '<1', '<5', '5.5 g'],[np.nan, '0', '0', '5.5'])\n\n# Confirm that the '<1','<5', and '5.5 g' are no longer in the dataframe using my previously defined function:\nprint('After fixing a number of the non-convertible items in the numeric columns: re-test to confirm they have been fixed: \\n')\ndf_non_numeric_list(file,['Company', 'Item'])\n\n# convert to float\nfile_no_company_item = file.drop(['Company', 'Item'], axis = 1)\nf_cols = file_no_company_item.columns\nfor i in f_cols:\n    file = file.astype({i : 'float'})\n\n# check for duplicates, and remove (there are 7 duplicates, just like I saw another notebook find)\nprint(len(file))\nfile = file.drop_duplicates()\nprint(len(file))\n\n# remove all rows where every value is NaN (except company/item). \nfile = file.dropna(subset = file_no_company_item.columns, how = 'all')\nprint(\"\\n I've also removed some of the NaN's, but some remain\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:27.934513Z","iopub.execute_input":"2023-09-15T18:03:27.935067Z","iopub.status.idle":"2023-09-15T18:03:28.052578Z","shell.execute_reply.started":"2023-09-15T18:03:27.935028Z","shell.execute_reply":"2023-09-15T18:03:28.050955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Addressing the remaining Null values in the DataFrame\nNow I will drop all the NaN's (there are relatively few, less than 5% of the rows in the dataframe) except the NaN's in the Weight Watchers Points column, which I'll leave as is, and the NaN's in the Calories from Fat column, which I will impute using the approximate of 9 calories / gram of fat. \n\nThe Weight Watchers column, from what I have seen, is exclusively NaN for some companies (so I didn't want to impute a mean WW Points to those), but as we'll see later, the WW Points column does not convey much useful information beyond the information already captured by the Total Calories column of the dataframe (the two values correlate at 0.99!). ","metadata":{}},{"cell_type":"code","source":"''' \nFirst, I will drop all the rows with NaNs below 5% of the columns (which is ~56, from 0.05*1126) then fill all the NaN's of the calories from fat with 9*total fat\nFinally I will choose to not impute the NaN's in the Weight Watchers column, as three of the companies simply have no data for that\n'''\n\ncols_to_drop_NaN = ['Total Fat\\n(g)', 'Saturated Fat\\n(g)', 'Trans Fat\\n(g)', 'Carbs\\n(g)', 'Fiber\\n(g)', 'Protein\\n(g)']\nfile = file.dropna(subset = cols_to_drop_NaN)\nfile['Calories from\\nFat'] = file['Calories from\\nFat'].fillna(9 * file['Total Fat\\n(g)'])\nprint('Have dropped all nulls except those in the Weight Watcher Points column: \\n')\nprint(file.isna().sum())\n\n# I can see the changes have taken effect -- now it time to finally plot something / move past data cleaning!","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:28.055678Z","iopub.execute_input":"2023-09-15T18:03:28.056091Z","iopub.status.idle":"2023-09-15T18:03:28.076599Z","shell.execute_reply.started":"2023-09-15T18:03:28.056055Z","shell.execute_reply":"2023-09-15T18:03:28.075055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Look at data: distribution of numerical variables","metadata":{}},{"cell_type":"code","source":"# Distributions of all the numerical data in the dataset\n\nfig, ax = plt.subplots(nrows = 3, ncols = 4, sharey = True)\ncol = 2\nfor axs in ax.flat:\n    axs.hist(file.iloc[:,col])\n    axs.set_title(file.columns[col].replace('\\n', ' ').replace('Weight Watchers', 'WW'),size = 9, y = 0.7)\n    axs.set_ylim(top = 1000)\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset\")\nplt.show()\n\n# boxplot\nfileWWdrop = file.dropna() \nfig, ax = plt.subplots(nrows = 3, ncols = 4, sharey = True)\ncol = 2\nfor axs in ax.flat:\n    if col == 13:\n        # Must use a version of the dataframe without Nll values, else the WW points won't plot\n        axs.boxplot(fileWWdrop.iloc[:,col])\n    else:\n        axs.boxplot(file.iloc[:,col])\n    axs.set_title(file.columns[col].replace('\\n', ' ').replace('Weight Watchers', 'WW'), size = 9, y = 1.0)\n    axs.set_ylim(top = 3000)\n    axs.set_xticklabels('')\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:28.078358Z","iopub.execute_input":"2023-09-15T18:03:28.078834Z","iopub.status.idle":"2023-09-15T18:03:30.803395Z","shell.execute_reply.started":"2023-09-15T18:03:28.078788Z","shell.execute_reply":"2023-09-15T18:03:30.801976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Potentially useful Transformation: cubed-root\n\nWith the except of the total calories and WW pnts columns, the distributions visible above are very right-skewed and have lots of values piled up near 0 with large tails of higher-value outliers, so a transformation of the data may be useful. \n\nI tried a log-transformation first, but settled on a root transformation because the many zero values in the dataset (even with the addition of a small number like 0.0001 to avoid infinities) seemed to throw off the output for some of the downstream analysis I was doing. A cubed-root ended up being simpler to execute and appeared to convert the data to relatively normal distributions for most columns, except for the pile of values remaining at 0 in many of them:","metadata":{}},{"cell_type":"code","source":"# Most of these are heavily right-skewed distributions, so let's transform them by taking the cubed root and see what happens:\nfile_re_trans = pd.DataFrame()\nfor i in file_no_company_item.columns:\n    file_re_trans[i] = (file[i]) ** (1/3)\n\n# Distributions of all the numerical data in the transformed dataset\n\nfig, ax = plt.subplots(nrows = 3, ncols = 4, sharey = True)\ncol = 0\nfor axs in ax.flat:\n    axs.hist(file_re_trans.iloc[:, col])\n    axs.set_title(file_re_trans.columns[col].replace('\\n', ' ').replace('Weight Watchers', 'WW'),size = 9, y = 0.7)\n    axs.set_ylim(top = 1000)\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset: cubed-root transformation\")\nplt.show()\n\n# boxplot\nfilere_WWdrop = file_re_trans.dropna() \nfig, ax = plt.subplots(nrows = 3, ncols = 4, sharey = True)\ncol = 0\nfor axs in ax.flat:\n    if col == 11:\n        axs.boxplot(filere_WWdrop.iloc[:, col])\n    else:\n        axs.boxplot(file_re_trans.iloc[:, col])    \n    axs.set_title(file_re_trans.columns[col].replace('\\n', ' ').replace('Weight Watchers', 'WW'), size = 9, y = 1.0)\n    axs.set_ylim(top = (3000 ** (1/3)))\n    axs.set_xticklabels('')\n    col += 1\nfig.suptitle(\"Distributions of Numerical Variables in Dataset: cubed-root transformation\")\nplt.show()\n\n# Much better! If called for, we can always use a cubed-root transformation, but the transformed dataframe currently doesn't have the first two columns: \nfile_re_trans['Company'] = file['Company']\nfile_re_trans['Item'] = file['Item'] ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:30.805557Z","iopub.execute_input":"2023-09-15T18:03:30.805943Z","iopub.status.idle":"2023-09-15T18:03:34.291326Z","shell.execute_reply.started":"2023-09-15T18:03:30.8059Z","shell.execute_reply":"2023-09-15T18:03:34.289513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributions of calories, sodium, and fiber by company","metadata":{}},{"cell_type":"code","source":"# Although there are still some outliers in the data, I want to move on to visualization and analysis for now \n\n# The question is -- what do we what to know?\n# I see other have split things up by company, so I'll look at that too\nfile['Company'].value_counts().plot(kind = 'bar')\nplt.title(\"Items in the Dataset from each Company\")\nplt.show()\n\nsns.catplot(kind = 'box', data = file, x = 'Company', y = 'Calories')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Calories by item from each company\")\nplt.show()\n\nsns.catplot(kind = 'box', data = file, x = 'Company', y = 'Sodium \\n(mg)')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Sodium by item from each company\")\nplt.show()\n\nsns.catplot(kind = 'box', data = file, x = 'Company', y = 'Fiber\\n(g)')\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of Fiber by item from each company\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:34.293393Z","iopub.execute_input":"2023-09-15T18:03:34.294831Z","iopub.status.idle":"2023-09-15T18:03:36.123187Z","shell.execute_reply.started":"2023-09-15T18:03:34.294773Z","shell.execute_reply":"2023-09-15T18:03:36.121262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Optional: print heatmaps of the correlations between columns at different calorie levels (<50,50-150,150-450,>450 total calories)\n","metadata":{}},{"cell_type":"code","source":"''' \n# but I also want to know about how many items are of different levels of calories, like sauces with very low calories, perhaps. I might use these later.\nxlow_cal = file[file['Calories'] <= 50]\nlow_cal = file[(file['Calories'] <= 150)  & (file['Calories'] > 50)]\nmed_cal = file[(file['Calories'] <= 450)  & (file['Calories'] > 150)]\nhigh_cal = file[file['Calories'] > 450]\nlisty = [xlow_cal, low_cal, med_cal, high_cal]\ntitle_list=['<50', '50-150', '150-450', '>450']\n\nfor ii,i in enumerate(listy):\n    corr = i.corr()\n    sns.heatmap(corr, annot = True, linewidth = 0.5,fmt = '0.2f')\n    plt.title('Correlations of {} calorie items'.format(title_list[ii]))\n    plt.show()\n'''\n\nprint('Uncomment code, or run directly by highlight --> ctrl+shift+enter for a lot of heatmaps across the various categories of calorie content')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:36.125494Z","iopub.execute_input":"2023-09-15T18:03:36.126665Z","iopub.status.idle":"2023-09-15T18:03:36.135343Z","shell.execute_reply.started":"2023-09-15T18:03:36.126521Z","shell.execute_reply":"2023-09-15T18:03:36.133655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation visualizations\n\nLet's see how the columns correlate with each other. This will also be useful for any downstream machine learning analysis, as we will be able to drop redundant columns.","metadata":{}},{"cell_type":"code","source":"\n# Let's see if we can predict company from the characteristics of the items:\n# First let's find highly correlated data and drop redundant/correlated columns\ncorr = file.corr()\nsns.heatmap(corr, annot = True, linewidth = 0.5, fmt = '0.2f')\nplt.title('Correlations of untransformed data, all columns')\nplt.show()\n\n\n# Almost all of the columns values are positively correlated (if weakly) with each other, except sugar\n# Now, some correlation comparisons:\n\n# Is the weight Watchers column just a proxy for total calories? will drop NaN rows\nfile_WW_comp = file.dropna(subset = 'Weight Watchers\\nPnts')\n\nsns.lmplot(data = file_WW_comp, x = 'Weight Watchers\\nPnts', y = 'Calories',line_kws = {'color':'r'})\nplt.title('Calories vs. Weight Watcher Points')\nplt.show()\n\n# Fat vs. Sugar\nsns.lmplot(data=file, x = 'Sugars\\n(g)', y = 'Total Fat\\n(g)', line_kws={'color':'r'})\nplt.title('Grams of Fat vs. Sugar')\nplt.show()\n\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:36.137061Z","iopub.execute_input":"2023-09-15T18:03:36.137488Z","iopub.status.idle":"2023-09-15T18:03:38.294537Z","shell.execute_reply.started":"2023-09-15T18:03:36.137424Z","shell.execute_reply":"2023-09-15T18:03:38.293167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion of correlation visualizations:\nWe can see that most measured values in the dataset correlate with each other positively, except sugar.\nInterestingly, when we look at the scatter plot of sugar vs. fat quantities, we can see that the negative correlation is driven by a large number of items that are either predominantly fat with no sugar, or vice versa, seen by the points tightly hugging the x and y axes in the final figure of this section.\n\nAnother point of interest is that the \"Weight Watchers Points\" is extremely tightly correlated with total calories (correlation = 0.99), perhaps indicating that the Weight Watcher Points system is not contributing additional information about fast food items beyond total calorie count.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Can we predict the company from the item characteristics?\n\nPerhaps we can use machine learning to train an algorithm to identify the company name from the nutrient characteristics of the item.","metadata":{}},{"cell_type":"code","source":"# Will use the cubed-root transformed data for this\n# All the fat columns & Cholesterol are highly correlated, drop all except Total Fat\nfile_drop = file_re_trans.drop(['Saturated Fat\\n(g)', 'Calories from\\nFat', 'Cholesterol\\n(mg)'], axis = 1)\n\n# Weight Watcher points correlates extremely closely with total calories, also was imputed for half of the companies in the dataset,: also drop\nfile_drop = file_drop.drop(['Weight Watchers\\nPnts'], axis = 1)\n\n# Let's do this only for the items > 50 Calories:\nfile_drop = file_drop[file_drop['Calories'] > (50 ** (1/3))]\n\n# Set random state for reproducibility\nSEED = 656\n\n# Prepare goal and predictive variables, and split data set into training and test portions:\nX = file_drop.drop(['Item', 'Company'], axis = 1)\n\ny = file_drop['Company']\ny = y.factorize()\ny = y[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = SEED)\n\n# let's see if we can identify companies from their items\n# Set up XGBoost and GridSearch algorithms:\nsc = StandardScaler()\nxg = xgb.XGBClassifier(objective = \"reg:linear\", random_state=SEED)\nparam = {\"max_depth\":[10, 20, 30],'n_estimators':[10,20,30],'gamma':[0.05, 0.15, 0.3]}\ncv = GridSearchCV(xg, param_grid = param, cv = 5)\n\n# Fit data to algorithms and test\nsc.fit_transform(X_train)\nsc.transform(X_test)\nmodel = cv.fit(X_train, y_train)\nbest = model.best_estimator_\nbest_param = model.best_params_\nscore = best.score(X_test, y_test)\nprint('For this model, I dropped items with total calories <50, dropped a number of redundant columns, and used the cubed-root transformed data. \\n')\naccuracy_if_guessing = file['Company'].value_counts().max() / len(file)\n\n\nprint('Best Scoring Model Accuracy:\\n', score, '\\n', f\"\\nAccuracy if guessing company using the mode (always guess McDonald's): {accuracy_if_guessing}\", '\\n\\n', 'Best Tested Parameters of the XGBoost model:', best_param)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-15T18:03:38.297991Z","iopub.execute_input":"2023-09-15T18:03:38.298379Z","iopub.status.idle":"2023-09-15T18:04:11.059063Z","shell.execute_reply.started":"2023-09-15T18:03:38.298345Z","shell.execute_reply":"2023-09-15T18:04:11.057807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not very good at predicting the company, with only ~63% accuracy! \n\nHowever this is still notably higher than the accuracy (31%) you might expect by chance (if you only guessed the most common company in the dataset -- McDonald's -- for each item). ","metadata":{}}]}